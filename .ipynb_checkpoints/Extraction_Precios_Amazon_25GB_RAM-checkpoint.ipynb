{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTO LA PÁGINA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\dti\\anaconda3\\lib\\site-packages (4.9.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\dti\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.0.1)\n",
      "Requirement already satisfied: scrapy in c:\\users\\dti\\anaconda3\\lib\\site-packages (2.5.0)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in c:\\users\\dti\\anaconda3\\lib\\site-packages (from scrapy) (1.5.0)\n",
      "Requirement already satisfied: Twisted[http2]>=17.9.0 in c:\\users\\dti\\anaconda3\\lib\\site-packages (from scrapy) (21.2.0)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in c:\\users\\dti\\anaconda3\\lib\\site-packages (from scrapy) (1.1.0)\n",
      "Requirement already satisfied: parsel>=1.5.0 in c:\\users\\dti\\anaconda3\\lib\\site-packages (from scrapy) (1.6.0)\n",
      "Requirement already satisfied: pyOpenSSL>=16.2.0 in c:\\users\\dti\\anaconda3\\lib\\site-packages (from scrapy) (19.1.0)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5; platform_python_implementation == \"CPython\" in c:\\users\\dti\\anaconda3\\lib\\site-packages (from scrapy) (2.0.5)\n",
      "Requirement already satisfied: lxml>=3.5.0; platform_python_implementation == \"CPython\" in c:\\users\\dti\\anaconda3\\lib\\site-packages (from scrapy) (4.5.2)\n",
      "Requirement already satisfied: service-identity>=16.0.0 in c:\\users\\dti\\anaconda3\\lib\\site-packages (from scrapy) (18.1.0)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in c:\\users\\dti\\anaconda3\\lib\\site-packages (from scrapy) (1.0.4)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in c:\\users\\dti\\anaconda3\\lib\\site-packages (from scrapy) (0.2.0)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in c:\\users\\dti\\anaconda3\\lib\\site-packages (from scrapy) (1.22.0)\n",
      "Requirement already satisfied: protego>=0.1.15 in c:\\users\\dti\\anaconda3\\lib\\site-packages (from scrapy) (0.1.16)\n",
      "Requirement already satisfied: cryptography>=2.0 in c:\\users\\dti\\anaconda3\\lib\\site-packages (from scrapy) (2.9.2)\n",
      "Requirement already satisfied: h2<4.0,>=3.0 in c:\\users\\dti\\anaconda3\\lib\\site-packages (from scrapy) (3.2.0)\n",
      "Requirement already satisfied: zope.interface>=4.1.3 in c:\\users\\dti\\anaconda3\\lib\\site-packages (from scrapy) (4.7.1)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\dti\\anaconda3\\lib\\site-packages (from Twisted[http2]>=17.9.0->scrapy) (19.3.0)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in c:\\users\\dti\\anaconda3\\lib\\site-packages (from Twisted[http2]>=17.9.0->scrapy) (21.0.0)\n",
      "Requirement already satisfied: Automat>=0.8.0 in c:\\users\\dti\\anaconda3\\lib\\site-packages (from Twisted[http2]>=17.9.0->scrapy) (20.2.0)\n",
      "Requirement already satisfied: incremental>=16.10.1 in c:\\users\\dti\\anaconda3\\lib\\site-packages (from Twisted[http2]>=17.9.0->scrapy) (17.5.0)\n",
      "Requirement already satisfied: constantly>=15.1 in c:\\users\\dti\\anaconda3\\lib\\site-packages (from Twisted[http2]>=17.9.0->scrapy) (15.1.0)\n",
      "Requirement already satisfied: twisted-iocpsupport~=1.0.0; platform_system == \"Windows\" in c:\\users\\dti\\anaconda3\\lib\\site-packages (from Twisted[http2]>=17.9.0->scrapy) (1.0.1)\n",
      "Requirement already satisfied: priority<2.0,>=1.1.0; extra == \"http2\" in c:\\users\\dti\\anaconda3\\lib\\site-packages (from Twisted[http2]>=17.9.0->scrapy) (1.3.0)\n",
      "Requirement already satisfied: six>=1.6.0 in c:\\users\\dti\\anaconda3\\lib\\site-packages (from parsel>=1.5.0->scrapy) (1.15.0)\n",
      "Requirement already satisfied: pyasn1 in c:\\users\\dti\\anaconda3\\lib\\site-packages (from service-identity>=16.0.0->scrapy) (0.4.8)\n",
      "Requirement already satisfied: pyasn1-modules in c:\\users\\dti\\anaconda3\\lib\\site-packages (from service-identity>=16.0.0->scrapy) (0.2.8)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in c:\\users\\dti\\anaconda3\\lib\\site-packages (from itemloaders>=1.0.1->scrapy) (0.10.0)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in c:\\users\\dti\\anaconda3\\lib\\site-packages (from cryptography>=2.0->scrapy) (1.14.0)\n",
      "Requirement already satisfied: hpack<4,>=3.0 in c:\\users\\dti\\anaconda3\\lib\\site-packages (from h2<4.0,>=3.0->scrapy) (3.0.0)\n",
      "Requirement already satisfied: hyperframe<6,>=5.2.0 in c:\\users\\dti\\anaconda3\\lib\\site-packages (from h2<4.0,>=3.0->scrapy) (5.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dti\\anaconda3\\lib\\site-packages (from zope.interface>=4.1.3->scrapy) (49.2.0.post20200714)\n",
      "Requirement already satisfied: idna>=2.5 in c:\\users\\dti\\anaconda3\\lib\\site-packages (from hyperlink>=17.1.1->Twisted[http2]>=17.9.0->scrapy) (2.10)\n",
      "Requirement already satisfied: pycparser in c:\\users\\dti\\anaconda3\\lib\\site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.0->scrapy) (2.20)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4\n",
    "!pip install scrapy\n",
    "import urllib.request\n",
    "import requests\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "#datos_gen = urllib.request.urlopen('http://www.coes.org.pe/portal/portalinformacion/generacion')\n",
    "#datos_dem=urllib.request.urlopen('http://www.coes.org.pe/portal/portalinformacion/demanda?txtFechaInicial=\"03/04/2020\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#precio $activo # link &page=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-235e9aeefc14>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0msoup3\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprettify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mxd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'span'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'a-size-medium a-color-base a-text-normal'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxd\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[1;31m#a = link.get_text().replace('\\n','')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlink\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'href'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "sess = requests.Session()\n",
    "url = \"https://www.amazon.com/s?k=laptops&language=es_US&ref=nb_sb_noss_2\"\n",
    "headers={\"User-Agent\":'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36'}\n",
    "page =requests.post(url, headers=headers)\n",
    "#soup = BeautifulSoup(page.content,'html.parser')\n",
    "\n",
    "soup1 = BeautifulSoup(page.content, \"html.parser\")\n",
    "soup2 = BeautifulSoup(soup1.prettify(), \"html.parser\")\n",
    "\n",
    "\n",
    "#title=soup.find_all(\"h2\",class_='a-size-mini a-spacing-none a-color-base s-line-clamp-2')\n",
    "\n",
    "### codigo\n",
    "dataframe1=pd.DataFrame(data={'Nombre':list(),'Link':list()})\n",
    "for link in soup2.findAll('a',class_='a-link-normal a-text-normal'):\n",
    "    soup3=BeautifulSoup(link.prettify(),'html.parser')\n",
    "    xd = soup3.findAll('span',class_='a-size-medium a-color-base a-text-normal')\n",
    "    a = xd[0].get_text().replace('\\n','')\n",
    "    #a = link.get_text().replace('\\n','')\n",
    "    c = link.get('href')\n",
    "    df = pd.DataFrame(data={'Nombre':[a],'Link':[c]})\n",
    "    dataframe1 = dataframe1.append(df)\n",
    "dataframe1.reset_index(drop=True, inplace=True)\n",
    "#soup2.findAll(class_='a-price-whole')\n",
    "\n",
    "dataframe2=pd.DataFrame(data={'Precio':list(),'Link':list()})\n",
    "for precio_link in soup2.findAll('a',class_='a-size-base a-link-normal a-text-normal'):\n",
    "    soup3=BeautifulSoup(precio_link.prettify(),'html.parser')\n",
    "    xd = soup3.findAll('span',class_='a-offscreen')\n",
    "    e = xd[0].get_text().replace('\\n','')\n",
    "    #e = precio_link.get_text()\n",
    "    f = precio_link.get('href')\n",
    "    df = pd.DataFrame(data={'Precio':[e],'Link':[f]})\n",
    "    dataframe2 = dataframe2.append(df)\n",
    "dataframe2.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "dataframe =pd.merge(dataframe1,dataframe2)\n",
    "#dataframe =dataframe1.merge(dataframe2,on='Link')\n",
    "dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.amazon.com/s?k=laptops&language=es_US&ref=nb_sb_noss_2&page='\n",
    "def amazon_pagina(url,x):\n",
    "    sess = requests.Session()\n",
    "    url = url + x\n",
    "    headers={\"User-Agent\":'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36'}\n",
    "    page =requests.post(url, headers=headers)\n",
    "    #soup = BeautifulSoup(page.content,'html.parser')\n",
    "\n",
    "    soup1 = BeautifulSoup(page.content, \"html.parser\")\n",
    "    soup2 = BeautifulSoup(soup1.prettify(), \"html.parser\")\n",
    "\n",
    "\n",
    "    #title=soup.find_all(\"h2\",class_='a-size-mini a-spacing-none a-color-base s-line-clamp-2')\n",
    "\n",
    "    ### codigo\n",
    "    dataframe1=pd.DataFrame(data={'Nombre':list(),'Link':list()})\n",
    "    for link in soup2.findAll('a',class_='a-link-normal a-text-normal'):\n",
    "        soup3=BeautifulSoup(link.prettify(),'html.parser')\n",
    "        xd = soup3.findAll('span',class_='a-size-medium a-color-base a-text-normal')\n",
    "        a = xd[0].get_text().replace('\\n','')\n",
    "        #a = link.get_text().replace('\\n','')\n",
    "        c = link.get('href')\n",
    "        df = pd.DataFrame(data={'Nombre':[a],'Link':[c]})\n",
    "        dataframe1 = dataframe1.append(df)\n",
    "    dataframe1.reset_index(drop=True, inplace=True)\n",
    "    #soup2.findAll(class_='a-price-whole')\n",
    "\n",
    "    dataframe2=pd.DataFrame(data={'Precio':list(),'Link':list()})\n",
    "    for precio_link in soup2.findAll('a',class_='a-size-base a-link-normal a-text-normal'):\n",
    "        soup3=BeautifulSoup(precio_link.prettify(),'html.parser')\n",
    "        xd = soup3.findAll('span',class_='a-offscreen')\n",
    "        e = xd[0].get_text().replace('\\n','')\n",
    "        #e = precio_link.get_text()\n",
    "        f = precio_link.get('href')\n",
    "        df = pd.DataFrame(data={'Precio':[e],'Link':[f]})\n",
    "        dataframe2 = dataframe2.append(df)\n",
    "    dataframe2.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    dataframe =pd.merge(dataframe1,dataframe2)\n",
    "    #dataframe =dataframe1.merge(dataframe2,on='Link')\n",
    "    return(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.amazon.com/s?k=laptops&language=es_US&ref=nb_sb_noss_2&page='\n",
    "dataamazon = pd.DataFrame(data={'Nombre':list(),'Link':list(),'Precio':list()})\n",
    "for i in range(20):\n",
    "    a = str(i+1)\n",
    "    dataframe = amazon_pagina(url,a)\n",
    "    dataamazon= dataamazon.append(dataframe)\n",
    "dataamazon.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataamazon.drop_duplicates(subset='Nombre', inplace=True )\n",
    "dataamazon.reset_index(drop=True, inplace=True)\n",
    "dataamazon[dataamazon.Nombre.str.contains(\"Lenovo\")]\n",
    "dataamazon.to_excel('dataamazon.xlsx',index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataamazon.loc[0].Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asd = dataframe1.iloc[3,1]\n",
    "we='https://www.amazon.com'+asd\n",
    "we"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe1.iloc[11,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link\n",
    "soup3=BeautifulSoup(link.prettify(),'html.parser')\n",
    "xd = soup3.findAll('span',class_='a-size-medium a-color-base a-text-normal')\n",
    "xd[0].get_text()\n",
    "soup3.findAll('href',class_='a-size-medium a-color-base a-text-normal')\n",
    "link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup2.findAll('a',class_='a-link-normal a-text-normal')[-1].get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precio_link\n",
    "soup3=BeautifulSoup(precio_link.prettify(),'html.parser')\n",
    "xd = soup3.findAll('span',class_='a-offscreen')\n",
    "xd[0].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#&page=20\n",
    "'https://www.amazon.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = requests.Session()\n",
    "url =\"https://www.amazon.com\" +\"/-/es/gp/slredirect/picassoRedirect.html/ref=pa_sp_atf_aps_sr_pg1_1?ie=UTF8&adId=A04052102LURNB4D4RAI6&url=%2FLenovo-IdeaPad-Processor-Graphics-81W0003QUS%2Fdp%2FB0862269YP%2Fref%3Dsr_1_1_sspa%3Fdchild%3D1%26keywords%3Dlaptop%26qid%3D1590174680%26sr%3D8-1-spons%26psc%3D1&qualifier=1590174680&id=3633383006645419&widgetName=sp_atf\"\n",
    "url ='https://www.amazon.com' + dataamazon.loc[1].Link\n",
    "headers={\"User-Agent\":'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36'}\n",
    "page =requests.post(url, headers=headers)\n",
    "soup = BeautifulSoup(page.content,'html.parser')\n",
    "\n",
    "#como no funciona solo con un soup se deben usar dos de https://www.youtube.com/watch?v=Bg9r_yLk7VY\n",
    "soup1 = BeautifulSoup(page.content, \"html.parser\")\n",
    "soup2 = BeautifulSoup(soup1.prettify(), \"html.parser\")\n",
    "\n",
    "title = soup2.find(id= \"productTitle\").get_text()\n",
    "price = soup2.find(id=\"priceblock_ourprice\").get_text()\n",
    "#para ver solo el nombre\n",
    "price\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url=\"http://www.coes.org.pe/portal/portalinformacion/demanda\"\n",
    "payload = {'fechaInicial': '06%2F04%2F2020', 'fechaFinal': '08%2F04%2F2020'}\n",
    "r2 = requests.post(url+'?'+json.dumps(payload))\n",
    "datos2 =r2.json()\n",
    "datos2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtengo los datos de la página web\n",
    "url = \"http://www.coes.org.pe/portal/portalinformacion/demanda\"\n",
    "query = requests.utils.urlparse(url)\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "sess = requests.Session()\n",
    "FI='06/04/2020'\n",
    "FF='08/04/2020'\n",
    "url = \"http://www.coes.org.pe/portal/portalinformacion/demanda?fechaInicial=\"+FI+\"&fechaFinal=\"+FF\n",
    "r = sess.post(url)\n",
    "#soup = BeautifulSoup(r.text)\n",
    "datos = r.json()\n",
    "data = datos['Data']\n",
    "df = pd.DataFrame(columns={'Fecha','Ejecutado','Prog. Diaria','Prog. Semanal'})\n",
    "for i in data:\n",
    "    fecha,ejec,diario,sem = [i['Fecha']],[i['ValorEjecutado']],[i['ValorProgramacionDiaria']],[i['ValorProgramacionSemanal']]\n",
    "    dfpequeño = pd.DataFrame(data={'Fecha':fecha,\n",
    "                                   'Ejecutado':ejec,\n",
    "                                   'Prog. Diaria':diario,\n",
    "                                   'Prog. Semanal':sem})\n",
    "    df = df.append(dfpequeño,ignore_index=True,sort=True)\n",
    "    \n",
    "df=df[['Fecha','Ejecutado','Prog. Diaria','Prog. Semanal']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(datos_dem)\n",
    "tags = soup('link')\n",
    "table=soup.find('table',class_='tabla-informacion') \n",
    "#table=soup.find('div',id='contenidoDemanda')\n",
    "soup.findAll('table')[0].findAll('tr')\n",
    "rows = table.find_all('tr')\n",
    "for row in rows:\n",
    "    print(row.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"url='http://www.coes.org.pe/portal/portalinformacion/demanda'\n",
    "requests.post(url=url, verify=False)\n",
    "html = requests.get(url,verify=False).content\n",
    "df_list = pd.read_html(html)\n",
    "df = df_list[3]\n",
    "df\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.item import Field"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
